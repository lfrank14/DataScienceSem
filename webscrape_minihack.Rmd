---
title: "Web Scraping Minihacks (Week 6)"
author: "Lea Frank"
date: "5/10/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, error = FALSE, warning = FALSE)
library(tidyverse)
library(ggthemes)
library(rvest)
library(stringr)
library(tidytext)
```

#Minihacks

##Minihack 1

Navigate to the nfl page that was used in the first example and pull the big plays count for each game. Once you have all of the data, report the mean and the standard deviation.  *If you are feeling ambitious, create a for loop that does this for each week of the season and store each mean and sd in a list.   


```{r minihack1}
# Automatically defining the number of weeks
link <- read_html("http://www.nfl.com/scores/2017/REG1")
week_item <- link %>% 
  html_nodes(".reg-season-games .week-item") %>% 
  html_text %>% 
  as.numeric
numweeks <- length(week_item)

# Defining the url to use in the for loop. Will define week number
url <- "http://www.nfl.com/scores/2017/REG"

# Creating an empty list to input big play count for each week
bigplays <- list()

for (i in 1:numweeks) {
  
  html <- str_c(url, as.character(i))
  link <- read_html(html)
  
  bpcount <- link %>% 
    html_nodes(".big-plays-count") %>% 
    html_text %>% 
    as.numeric
  
  bigplays[[i]] <- data.frame(week = as.character(i), bpcount = bpcount)

}

# Unpack the contents of the list and append to a new dataframe, already in tidy format
bpcount <- data.frame()
for (i in 1:numweeks) {
  bpcount <- bind_rows(bpcount, bigplays[[i]])
}

bpcount_sum <- bpcount %>% 
  group_by(week) %>% 
  summarize(m_count = mean(bpcount),
            sd_count = sd(bpcount))
bpcount_sum$week <- as.numeric(bpcount_sum$week)
bpcount_sum <- arrange(bpcount_sum, week)

bpcount_sum %>% 
  ggplot(aes(x = week, y = m_count, fill = week)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  theme_hc()
```



##Minihack 2

Go back to the TrustPilot website and look at the reviews.  You’ll notice that there is information on each review about the number of reviews the author has posted on the website (Ex: Joey Perry, 4 reviews).  Write a function that, for a given webpage,  gets the number of reviews each reviewer has made.  

If you are having trouble finding the corresponding CSS tag, ask a classmate!  Note as well that only pulling the number of other reviews will involve some text manipulation to get rid of irrelevant text.

At the end you should have a function that takes an html object as a parameter and returns a numeric vector of length 20. 


```{r minihack2}
rm(list = ls())

url <- "https://www.trustpilot.com/review/united-airlines.com"

get_numreviews <- function(url) {
  
  # create html object
  link <- read_html(url)
  
  # extract name of reviewer
  author <- link %>% 
    html_nodes(".consumer-info__details__name") %>% 
    html_text() %>% 
    str_trim()
  
  # extract number of reviews per reviewer
  numreviews <- link %>% 
    html_nodes(".consumer-info__details__review-count") %>% 
    html_text()
  numreviews <- numreviews %>% 
    str_trim() %>% 
    str_extract(pattern = "\\d")
  
  author_numreviews <- data.frame(author = author, numreviews = numreviews)
  #print(author_numreviews)
}

author_numreviews <- get_numreviews(url)

```


##Minihack 3

The web is a vast ocean of data that is waiting to be scraped. For this hack, be creative. Find a website of your choice and pull some data that you find interesting. Tidy your data and print the head of your dataframe. Perform some simple descriptive statistics and if you’re feeling ambitious create at least one visualization of the trends that you have discovered. There are no limitations to what kind of data you decide to pull (but don’t forget our initial disclaimer!). It can be numbers and it can also be text if you decide that you want to use the skills from the text processing lesson that we covered last week. 

If you don’t know where to start, scraping from imdb.com, espn.go.com, news sites, bestplaces.net,  is sufficient. 

Lea's Comments:
In this minihack I *attempted* to create a way to scrape information from a PubMed literature search. I was able to create a function that scraped the title, journal, author, and abstract from a search using just the defined key search terms. Unfortunately, I'm still trying to figure out how to jump to the next page of results, as the css for the next button isn't very clear. Another thing I'm having trouble with is extracting the date of publication, as it's buried in a long string of text. Ultimately, the idea is to have a way to narrow down a search relevant articles that may have pages and pages of results. 


```{r minihack3}
rm(list = ls())

# In this hack I will atempt to automate a literature search on PubMed that helps weed through some of the abstracts for relevant content.

# Define key terms to include in search
key_terms <- c("hippocampus", "anterior", "posterior", "connectivity")
#date <- could possibly include a search criteria like date 

pubmed_litsearch <- function(key_terms) {
  
  # create search terms
  search_terms <- vector()
  for (i in 1:length(key_terms)) {
    search_terms <- str_c(search_terms, key_terms[i], sep = "+")
  }
  
  # create link
  search_link <- str_c("https://www.ncbi.nlm.nih.gov/pubmed/?term=", search_terms, sep = "")
  
  # read html
  link <- html_session(search_link)
  
  # find total number of pages
  pages <- link %>% 
    html_nodes("#pageno") %>% 
    html_attr(name = "last") %>% 
    as.numeric()
  
  # collect article information from each search page - not working right now because I can't figure out the css link to the next page of results
  #while (a < pages) { 
    
    title <- link %>% 
      html_nodes(".title") %>% 
      html_text() %>% 
      str_trim() %>% 
      str_to_lower()
    
    author <- link %>% 
      html_nodes(".desc") %>% 
      html_text() %>% 
      str_trim() %>% 
      str_to_lower()
    
    journal <- link %>% 
      html_nodes(".details .jrnl") %>% 
      html_attr(name = "title") %>% 
      str_trim() %>% 
      str_to_lower()
  
    # pubmed id
    pm_id <- link %>% 
      html_nodes(".rprtnum input") %>% 
      html_attr(name = "value")
    
    # create dataframe with article info (trying to figure out how to get data, it's buried in text)
    article_info <- data.frame(pm_id, author, title, journal)
    
    # go to each article page and extract more information and abstract
    per_page <- length(title)
    abstracts <- list()
    
    for (i in 1:per_page) {
      
      article_link <- str_c("https://www.ncbi.nlm.nih.gov/pubmed/",pm_id[i])
    
      article_link <- read_html(article_link)
      
      doi <- article_link %>% 
        html_nodes(".resc dd a") %>% 
        html_attr(name = "href") %>% 
        str_remove("//doi.org/")
      
      # some articles have PMCID which interferes with calling the DOI number
      doi <- ifelse(length(doi) == 2, doi[2], doi)
      
      abstr <- article_link %>% 
        html_nodes(".abstr p") %>% 
        html_text() %>% 
        str_to_lower() %>% 
        paste(collapse = "") #some abstracts had multiple sections so collapsed into single vector
      
      tmp <- data.frame(pm_id = pm_id[i], doi, abstract = abstr)
      abstracts[[i]] <- tmp
    }
    
    abstract <- map_dfr(abstracts, data.frame)
    
    # Combine abstract info with other info
    article_info <- left_join(article_info, abstract)
    
    
    # getting stuck trying to navigate to next page. no way to enter link manually 
    linktmp <- link %>% 
      #html_nodes(".title_and_pager")
      follow_link("Next page of results")
      
  #}
}

litsearch <- pubmed_litsearch(key_terms = key_terms)
```

```{r minihack3_part2}
# Most relevant journals
litsearch %>% 
  group_by(journal) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = reorder(journal, n), y = n, fill = journal)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_flip()


# Text analysis on abstract
litsearch_abs <- litsearch %>% 
  unnest_tokens(output = word, input = abstract) %>% 
  group_by(word) %>% 
  summarize(n = n())

data(stop_words)

litsearch_abs <- litsearch_abs %>% 
  anti_join(stop_words)

litsearch_abs$word <- str_remove_all(litsearch_abs$word, "\\d")
litsearch_abs <- litsearch_abs %>% 
  filter(str_detect(word, "\\w"))

litsearch_abs$word <- reorder(litsearch_abs$word, litsearch_abs$n)

litsearch_abs %>% 
  top_n(10) %>% 
  ggplot(aes(x = word, y = n, fill = word)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_flip()
```


Helpful Sources:

[Hadley Wickham’s rvest](http://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/)

[RPubs Web Scraping Tutorial](https://rpubs.com/ryanthomas/webscraping-with-rvest)

[CRAN Selectorgadget](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html)
